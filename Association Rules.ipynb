{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25e25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f881f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apyori'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapyori\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apriori\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'apyori'"
     ]
    }
   ],
   "source": [
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f8681-19a9-43eb-9b76-91bb83d723af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\singh\\OneDrive\\Desktop\\datascience\\Association Rules\\Online retail.xlsx',header = None, names = ['Items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data['Items'].str.split(\",\", expand = True)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e21c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing all the none value with NaN as NaN works good with mathematical logics.\n",
    "data1=data1.fillna('NaN')\n",
    "data1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we need a data in form of list for Apriori Algorithm.\n",
    "\n",
    "records = []\n",
    "for i in range(1, 7501):\n",
    "    records.append([str(data1.values[i, j]) for j in range(0, 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f612e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(type(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f948bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori Algorithm\n",
    "# Now time to apply algorithm on data.\n",
    "\n",
    "#We have provide min_support, min_confidence, min_lift, and min length of sample-set for find rule.\n",
    "association_rules = apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)\n",
    "association_results = list(association_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b31ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of relation derived\n",
    "print(\"There are {} Relation derived.\".format(len(association_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association Rules Derived\n",
    "for i in range(0, len(association_results)):\n",
    "    print(association_results[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules Generated\n",
    "for item in association_results:\n",
    "    # first index of the inner list\n",
    "    # Contains base item and add item\n",
    "    pair = item[0]\n",
    "    items = [x for x in pair]\n",
    "    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n",
    "\n",
    "    # second index of the inner list\n",
    "    print(\"Support: \" + str(item[1]))\n",
    "\n",
    "    # third index of the list located at 0th\n",
    "    # of the third index of the inner list\n",
    "\n",
    "    print(\"Confidence: \" + str(item[2][0][2]))\n",
    "    print(\"Lift: \" + str(item[2][0][3]))\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5edace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to computational inefficiency using smaller dataset\n",
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f95e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33847c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_try = te.fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(te_try, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56592559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training\n",
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30601a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori(df,min_support=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167cd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training with Column Result return\n",
    "\n",
    "apriori(df,min_support=0.5, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b11f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the length of Itemset\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length is 2 and Support is > 0.8\n",
    "frequent_itemsets[ (frequent_itemsets['length'] == 2) & (frequent_itemsets['support'] >= 0.8) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e472e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[ frequent_itemsets['itemsets'] == {'Onion', 'Eggs'} ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verbose return the number of iteration and itemset default size\n",
    "apriori(df, min_support=0.6, use_colnames=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293efdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Max_len set the itemset\n",
    "apriori(df, min_support=0.6, use_colnames=True, verbose=1, max_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b9bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interview Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ef0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\tWhat is lift and why is it important in Association rules?\n",
    "\n",
    "\n",
    "# Lift: Uncovering Unexpected Associations\n",
    "# In association rule learning, lift is a crucial metric that goes beyond simple co-occurrence of items. While support and confidence tell you how often items appear together, lift helps determine if an association is surprising or interesting compared to an assumption of independence between items.\n",
    "\n",
    "# Importance of Lift:\n",
    "\n",
    "#Lift is crucial because support and confidence alone can be misleading:\n",
    "\n",
    "# High Support: A high support value doesn't necessarily imply a strong association. Frequent items might appear together by chance in many transactions.\n",
    "# High Confidence: Similarly, a high confidence value tells you that if items in the \"if\" part are present, the \"then\" part is likely to be present as well. However, it doesn't tell you how unexpected this association is.\n",
    "\n",
    "# By considering lift, you can filter out trivial associations and focus on rules that reveal truly interesting or unexpected relationships between items. For example:\n",
    "\n",
    "# Rule: \"If a customer buys bread, they are also likely to buy milk\" (High support, high confidence). This might not be surprising since people often buy bread and milk together.\n",
    "# Rule: \"If a customer buys peanut butter, they are also likely to buy bananas\" (Moderate support, high lift). This association might be more unexpected and potentially interesting for product placement strategies.\n",
    "\n",
    "# Using Lift in Association Rule Learning:\n",
    "\n",
    "# Set a minimum lift threshold: Rules with lift values below this threshold can be considered uninteresting and discarded.\n",
    "# Focus on rules with high support and high lift to discover strong and unexpected associations.\n",
    "# Lift helps you identify patterns that go beyond chance co-occurrence, providing valuable insights for marketing, recommendation systems, and other applications that involve analyzing item relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\tWhat is support and Confidence. How do you calculate them?\n",
    "\n",
    "#  Support:\n",
    "# Support measures the proportion of transactions in a dataset that contain both the \"if\" part (antecedents) and the \"then\" part (consequents) of an association rule. It represents how frequently the complete itemset (all items in the rule) appears together.\n",
    "\n",
    "# Calculation of Support:\n",
    "# Support = (Number of transactions containing {antecedents} and {consequents}) / (Total number of transactions)\n",
    "\n",
    "#  Confidence:\n",
    "\n",
    "# Confidence measures the probability of finding the consequents (then part) in a transaction, given that the antecedents (if part) are already present. It represents how reliable the rule is in predicting the \"then\" part based on the \"if\" part.\n",
    "\n",
    "# Calculation of Confidence:\n",
    "# Confidence = (Number of transactions containing {antecedents} and {consequents}) / (Number of transactions containing {antecedents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddbea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\tWhat are some limitations or challenges of Association rules mining?\n",
    "\n",
    "# Association rule mining (ARM) is a valuable technique for uncovering interesting relationships between items in a dataset. However, it also comes with some limitations and challenges that you should be aware of:\n",
    "\n",
    "# 1. Identifying Interesting Rules:\n",
    "\n",
    "# High Volume of Rules: ARM algorithms can generate a large number of rules, making it difficult to identify the truly relevant and actionable ones. Setting appropriate minimum support and confidence thresholds can help mitigate this, but fine-tuning these parameters requires experimentation.\n",
    "# Identifying Unexpected Relationships: ARM focuses on frequent co-occurrences, but a high support or confidence doesn't necessarily indicate a surprising or interesting association. Lift helps in this regard, but defining a meaningful lift threshold can be subjective.\n",
    "\n",
    "# 2. Data Quality and Bias:\n",
    "\n",
    "# Inaccurate or Incomplete Data: The quality of the data significantly impacts the quality of the results. Noisy, inconsistent, or missing data can lead to misleading or unreliable rules. Data cleaning and preprocessing are crucial.\n",
    "# Bias in Data: If the data reflects inherent biases or user preferences, the discovered rules might perpetuate those biases. It's important to be aware of potential biases and interpret the results cautiously.\n",
    "\n",
    "# 3. Computational Complexity:\n",
    "\n",
    "# Large Datasets: Applying ARM algorithms to very large datasets can be computationally expensive. This might require specialized hardware or optimized algorithms, especially for real-time processing.\n",
    "# Handling Complex Relationships: ARM struggles to capture complex relationships between items that go beyond simple co-occurrence. Techniques like sequential pattern mining or advanced clustering approaches might be better suited for these scenarios.\n",
    "\n",
    "# 4. Overfitting and Generalizability:\n",
    "\n",
    "# Overfitting to Specific Data: Setting overly strict minimum support or confidence thresholds might lead to rules that are highly specific to the training data but don't generalize well to unseen data. It's essential to test and validate the rules on a separate holdout set.\n",
    "# Limited Scope: ARM typically focuses on binary relationships (presence or absence of items). It might miss out on more nuanced relationships or patterns involving quantities, temporal dependencies, or other attributes.\n",
    "\n",
    "# 5. Interpretability of Results:\n",
    "\n",
    "# Complexity of Rules: Longer association rules with multiple antecedents or consequents can be harder to interpret and understand the meaning behind the relationships.\n",
    "# Domain Knowledge: Effectively utilizing the discovered rules often requires domain knowledge about the context and potential meaning of the relationships.\n",
    "\n",
    "# In Conclusion:\n",
    "\n",
    "# While association rule mining is a powerful technique, being aware of its limitations and challenges is essential. By carefully considering these factors, you can improve the quality and interpretability of the results, leading to more meaningful insights from your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ec967-1e7f-4f88-a342-fa8adb2b4714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d870b-9604-4122-b1eb-c01bd6cc6bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
